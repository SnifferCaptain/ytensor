# Property Queries

This document describes the property query methods in `YTensor`, which are used to inspect the memory layout and shape characteristics of a tensor.

---

## Overview

`YTensor` provides the following property query methods:

- **`isContiguous()`**: Checks if the tensor is stored contiguously in memory.
- **`isContiguousFrom()`**: Gets the starting dimension from which the tensor is contiguous.
- **`isDisjoint()`**: Checks if the tensor's elements are non-overlapping in memory.
- **`shapeMatch()`**: Checks if the tensor's shape matches a given shape.

These methods are used for:
- **Optimization**: Selecting fast algorithm paths (e.g., SIMD) for contiguous tensors.
- **Safety**: Ensuring memory safety by avoiding modifications on overlapping elements.
- **Validation**: Verifying shapes before operations like broadcasting.

---

### Parameters

| Name | Type | Default Value | Description |
| --- | --- | --- | --- |
| `fromDim` | `int` | `0` | The starting dimension index (supports negative indexing). |

### Core Function Description

Checks if the tensor is stored contiguously in memory starting from a specified dimension.
Only checks dimensions where `_shape[i] > 1` (dimensions of size 1 do not affect contiguity).

### Return Value

- **`true`**: All dimensions starting from `fromDim` have logical strides that match their physical strides.
- **`false`**: At least one dimension has a logical stride $\neq$ physical stride.

### Internal Logic

```cpp
auto logStride = this->stride();  // Logical strides
for (int i = fromDim; i < dim; ++i) {
    if (logStride[i] != _stride[i] && _shape[i] > 1) {
        return false;  // Non-contiguous
    }
}
return true;  // Contiguous
```

**Note**: The check ignores dimensions where `_shape[i] == 1` as they do not affect overall memory contiguity.

### Usage Example

#### Basic Usage

```cpp
#include "ytensor_single.hpp"

int main() {
    // New tensor: Contiguous
    auto a = yt::YTensor<float, 3>::randn(2, 3, 4);
    std::cout << a.isContiguous() << std::endl;  // Result: 1 (true)
    
    // Transpose: Typically non-contiguous
    auto b = a.transpose(0, 1);
    std::cout << b.isContiguous() << std::endl;  // Result: 0 (false)
    
    // Slice: Might be non-contiguous
    auto c = a.slice({{}, {}, {0, 4, 2}});  // Every other column
    std::cout << c.isContiguous() << std::endl;  // Result: 0 (false)
    
    return 0;
}
```

#### Partial Contiguity Check

```cpp
auto a = YTensor<float, 3>::randn(2, 3, 4);
auto b = a.transpose(0, 1);  // Shape (3, 2, 4)

// b._stride = {4, 12, 1}
// b.stride() = {8, 4, 1}

std::cout << b.isContiguous(0) << std::endl;  // false (Non-contiguous from dim 0)
std::cout << b.isContiguous(1) << std::endl;  // false (Non-contiguous from dim 1)
std::cout << b.isContiguous(2) << std::endl;  // true (Dimension 2 is contiguous)
```

#### Algorithm Path Selection

```cpp
template <typename T, int dim>
void process(const YTensor<T, dim>& tensor) {
    if (tensor.isContiguous()) {
        // âœ… Fast path: use SIMD or sequential memory access
        const T* data = tensor.data_();
        for (size_t i = 0; i < tensor.size(); i++) {
            // Direct sequential access
            process_element(data[i]);
        }
    } else {
        // âŒ Slow path: use coordinate-based access
        for (size_t i = 0; i < tensor.size(); i++) {
            auto coord = tensor.toCoord(i);
            process_element(tensor.at(coord));
        }
    }
}
```

---

### Function Signature

```cpp
int isContiguousFrom() const;
```

### Core Function Description

Gets the starting dimension from which the tensor is contiguous.
Returns the index of the first dimension where "all subsequent dimensions are contiguous".

### Return Value

- Returns the index of the first dimension from which **all subsequent dimensions** are contiguous.
- Returns `ndim()` if the tensor is entirely non-contiguous.
- Returns `0` if the entire tensor is contiguous.

### Internal Logic

```cpp
auto logStride = this->stride();
for (int a = ndim() - 1; a >= 0; --a) {
    if (logStride[a] != _stride[a] && _shape[a] > 1) {
        return a + 1;  // Contiguous starting from index a + 1
    }
}
return 0;  // Fully contiguous
```

The check proceeds **backward** from the last dimension to find the first discrepancy.

### Usage Example

#### Basic Usage

```cpp
auto a = YTensor<float, 3>::randn(2, 3, 4);

// Fully contiguous
std::cout << a.isContiguousFrom() << std::endl;  // Result: 0

// Transpose (0, 1): Only the last dimension remains contiguous
auto b = a.transpose(0, 1);
std::cout << b.isContiguousFrom() << std::endl;  // Result: 2

// Slice the last dimension: Entirely non-contiguous
auto c = a.slice({{}, {}, {0, 4, 2}});
std::cout << c.isContiguousFrom() << std::endl;  // Result: 3
```

#### Relationship with `isContiguous()`

```cpp
auto a = YTensor<float, 3>::randn(2, 3, 4);
auto b = a.transpose(0, 1);

int from = b.isContiguousFrom();  // from = 2

// Equivalent check
assert(b.isContiguous(from) == true);  // Contiguous from index 2
if (from > 0) {
    assert(b.isContiguous(from - 1) == false);  // Non-contiguous from index 1
}
```

---

### Function Signature

```cpp
bool isDisjoint() const;
```

### Core Function Description

Checks if the tensor's elements are **disjoint** (i.e., no two distinct logical coordinates point to the same physical memory location).
If a stride is less than the dimension's minimum span, it is considered potentially overlapping.
Broadcasted tensors are typically overlapping.

### Return Value

- **`true`**: All logical elements occupy unique positions in memory (no overlap).
- **`false`**: At least two elements share the same memory location (overlap).

### Internal Logic

```cpp
// Calculate the minimum required span for each dimension (span_lower)
std::vector<int> span_lower(dim, 1);
for (int d = dim - 2; d >= 0; --d) {
    span_lower[d] = span_lower[d + 1] * _shape[d + 1];
}

// Check each dimension
for (int d = 0; d < dim; ++d) {
    if (_shape[d] <= 1) continue;
    int abs_stride = std::abs(_stride[d]);
    if (abs_stride < span_lower[d]) {
        return false;  // Overlapped
    }
}
return true;  // Disjoint
```

**Core Idea**: If the stride of a dimension is smaller than the theoretical minimum span required by the inner dimensions, elements must overlap.

### Usage Example

#### Basic Usage

```cpp
auto a = YTensor<float, 2>::randn(3, 4);

// Standard tensor: Disjoint
std::cout << a.isDisjoint() << std::endl;  // Result: 1 (true)

// Broadcasted tensor: Non-disjoint
auto b = a.slice({{}, {0, 1}});  // Take the first column (3, 1)
auto c = b.expand(std::vector<int>{3, 4});  // Broadcast to (3, 4)
// c._stride = {4, 0}  <- Stride 0 means all columns share the same memory
std::cout << c.isDisjoint() << std::endl;  // Result: 0 (false)
```

#### Detecting `stride=0` Broadcasting

```cpp
auto a = YTensor<float, 1>::randn(10);
auto b = a.reshape({10, 1});  // (10, 1)
auto c = b.expand(std::vector<int>{10, 5});  // Broadcast to (10, 5)

// c._shape = {10, 5}
// c._stride = {1, 0}  <- Dimension 1 has stride 0

std::cout << c.isDisjoint() << std::endl;  // false

// Modifying c in-place is unsafe as it affects multiple logical positions
// c.at(0, 0) = 1.0f;
// c.at(0, 1) will also become 1.0f due to memory sharing
```

---

### Function Signature

```cpp
bool shapeMatch(const std::vector<int>& otherShape) const;
```

### Parameters

| Name | Type | Description |
| --- | --- | --- |
| `otherShape` | `const std::vector<int>&` | The shape vector to compare against. |

### Core Function Description

Checks if the tensor's shape matches a provided shape vector.
Only compares the number of dimensions and the size of each dimension, not data types or strides.

### Return Value

- **`true`**: Tensors have the same number of dimensions and each dimension matches in size.
- **`false`**: There is a mismatch in internal rank or dimension sizes.

### Usage Example

#### Basic Usage

```cpp
auto a = YTensor<float, 3>::randn(2, 3, 4);

// Matching shape
std::cout << a.shapeMatch({2, 3, 4}) << std::endl;  // Result: 1 (true)

// Mismatched shapes
std::cout << a.shapeMatch({2, 3}) << std::endl;     // false (Different ndim)
std::cout << a.shapeMatch({2, 3, 5}) << std::endl;  // false (Different size)
std::cout << a.shapeMatch({4, 3, 2}) << std::endl;  // false (Different order)
```

#### Validation Before Broadcasting

```cpp
template <typename T, int dim>
YTensor<T, dim> add_inplace(YTensor<T, dim>& a, const YTensor<T, dim>& b) {
    if (!a.shapeMatch(b.shape())) {
        throw std::invalid_argument(
            "Shape mismatch! Cannot add tensors with different shapes."
        );
    }
    
    for (size_t i = 0; i < a.size(); i++) {
        a.atData_(i) += b.atData_(i);
    }
    
    return a;
}
```

---

## Combining Property Queries

### Choosing the Most Efficient Algorithm

```cpp
template <typename T, int dim>
void element_wise_op(YTensor<T, dim>& a, const YTensor<T, dim>& b) {
    if (!a.shapeMatch(b.shape())) {
        throw std::invalid_argument("Shape mismatch");
    }
    
    if (a.isContiguous() && b.isContiguous()) {
        // âœ… Fastest: Both are contiguous, use SIMD-friendly loop
        T* a_data = a.data_();
        const T* b_data = b.data_();
        #pragma omp simd
        for (size_t i = 0; i < a.size(); i++) {
            a_data[i] += b_data[i];
        }
    } else if (a.isDisjoint() && b.isDisjoint()) {
        // ðŸ”¶ Medium: Non-contiguous but disjoint, use standard index-based access
        for (size_t i = 0; i < a.size(); i++) {
            auto coord = a.toCoord(i);
            a.at(coord) += b.at(coord);
        }
    } else {
        // âŒ Slowest: Data overlaps, requires a temporary buffer or safe copying
        auto tmp = a.clone();
        for (size_t i = 0; i < a.size(); i++) {
            auto coord = a.toCoord(i);
            tmp.at(coord) = a.at(coord) + b.at(coord);
        }
        a = std::move(tmp);
    }
}
```

---

## Common Layout Patterns

| Operation | `isContiguous()` | `isContiguousFrom()` | `isDisjoint()` |
| --- | --- | --- | --- |
| `YTensor(2, 3, 4)` | âœ… true | 0 | âœ… true |
| `transpose(0, 1)` | âŒ false | 2 | âœ… true |
| `slice({}, {}, {0, 4, 2})` | âŒ false | 3 | âœ… true |
| `reshape({6, 4})` | âœ… true (if start was) | 0 | âœ… true |
| `expand({2, 3, 4})` | âŒ false | varies | âŒ false |

### Special Cases

#### Unit Dimensions (Size 1)

Dimensions with size $1$ do not affect contiguity:

```cpp
auto a = YTensor<float, 3>::randn(1, 3, 4);
std::cout << a.isContiguous() << std::endl;  // true

auto b = a.transpose(0, 1);  // Shape (3, 1, 4)
std::cout << b.isContiguous() << std::endl;  // true (Size-1 dimension doesn't break layout)
```

#### Scalars and 1D Tensors

One-dimensional tensors (unless sliced with a step) are always contiguous:

```cpp
auto a = YTensor<float, 1>::randn(100);
std::cout << a.isContiguous() << std::endl;       // true
std::cout << a.isContiguousFrom() << std::endl;  // 0
std::cout << a.isDisjoint() << std::endl;        // true
```

#### Uninitialized Tensors

Empty or uninitialized tensors are generally treated as non-contiguous:

```cpp
auto a = YTensor<float, 2>();
std::cout << a.isContiguous() << std::endl;       // false
std::cout << a.isContiguousFrom() << std::endl;  // ndim (2)
std::cout << a.isDisjoint() << std::endl;        // false
```

---

## Best Practices

1. **Performance Critical Paths**: Always check for contiguity to leverage faster algorithm paths.
2. **In-place Modifications**: Verify that the tensor is disjoint to prevent unintended memory corruption due to shared elements.
3. **Shape Validation**: Use `shapeMatch()` for clean and readable verification of tensor compatibility.
4. **Debugging**: Combine these queries to fully understand the internal state of complex tensor views.

---

## Related Content

- [Stride Queries](../shape/query.mdx) - Methods like `stride()` and `stride_()`.
- [Memory Operations](../construction/memory.mdx) - Methods like `contiguous()` and `clone()`.
- [Broadcasting](../math/broadcast.mdx) - Expansion rules and logic.
- [Indexing Access](../access/indexing.mdx) - Methods `at()` and `atData()`.
