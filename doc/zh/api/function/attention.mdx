# 注意力机制

本文档介绍 `yt::function` 命名空间中的注意力机制函数。

## scaledDotProductAttention

### 核心功能描述

`scaledDotProductAttention` 实现标准的 Scaled Dot-Product Attention。

这个函数实际上代表的是 *Attention Is All You Need* 论文中的 Scaled Dot-Product Attention 算子。

其标准数学公式为：

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

在此处，`scale` 参数对应公式中的 $\frac{1}{\sqrt{d_k}}$（通常为 `1 / sqrt(head_dim)`）。如果提供了 mask，则在 softmax 之前应用 mask（通常将 mask 位置设为负无穷）。

等价于执行以下步骤：
1. 计算 $S = Q \times K^T \times \text{scale}$
2. 若有 mask，则 $S = S + \text{mask}$
3. 计算 $P = \text{softmax}(S)$
4. 输出 $O = P \times V$

## 函数签名

```cpp
template<typename T, int dim>
YTensor<T, dim> scaledDotProductAttention(
    YTensor<T, dim>& query,        // [..., n0, c0]
    YTensor<T, dim>& key,          // [..., n1, c0]
    YTensor<T, dim>& value,        // [..., n1, c1]
    T scale = static_cast<T>(0.0),
    YTensor<T, 2>* mask = nullptr,
    sdpaBackend backend = sdpaBackend::MATH
);
```

## 参数说明

| 参数名 | 类型 | 默认值 | 说明 |
| --- | --- | --- | --- |
| `query` | `YTensor<T, dim>&` | - | Query 张量，形状 [..., n0, c0] |
| `key` | `YTensor<T, dim>&` | - | Key 张量，形状 [..., n1, c0] |
| `value` | `YTensor<T, dim>&` | - | Value 张量，形状 [..., n1, c1] |
| `scale` | `T` | `0.0` | 缩放因子。若为 0，自动计算为 1/√c0 |
| `mask` | `YTensor<T, 2>*` | `nullptr` | 可选的注意力掩码，形状 [n0, n1] |
| `backend` | `sdpaBackend` | `MATH` | 计算后端 |

## sdpaBackend 枚举

```cpp
enum struct sdpaBackend {
    MATH  // 标准数学实现
};
```

## 返回值

- **类型**: `YTensor<T, dim>`
- **形状**: `[..., n0, c1]`

## 使用示例

```cpp
#include "ytensor_function.hpp"

void example() {
    // 批量注意力：batch=2, heads=4, seq_len=10, d_k=64
    auto query = yt::YTensor<float, 4>::randn({2, 4, 10, 64});
    auto key = yt::YTensor<float, 4>::randn({2, 4, 10, 64});
    auto value = yt::YTensor<float, 4>::randn({2, 4, 10, 64});
    
    // 标准注意力
    auto output = yt::function::scaledDotProductAttention(query, key, value);
    // output.shape() -> {2, 4, 10, 64}
    
    // 带掩码的注意力
    auto mask = yt::YTensor<float, 2>::zeros({10, 10});
    // 设置因果掩码（下三角）
    for (int i = 0; i < 10; ++i) {
        for (int j = i + 1; j < 10; ++j) {
            mask.at(i, j) = -1e9f;  // 负无穷
        }
    }
    
    auto causal_output = yt::function::scaledDotProductAttention(
        query, key, value, 0.0f, &mask
    );
}
```

## 计算过程

1. **Score**: `QK^T = matmul(query, key.transpose())`
2. **Scale**: `QK^T *= scale`
3. **Mask** (可选): `QK^T += mask`
4. **Softmax**: `softmax_(QK^T, -1)`
5. **Output**: `matmul(softmax_result, value)`

---

## 相关内容

- [矩阵乘法](./matmul.mdx) - matmul 函数
- [Softmax](./softmax.mdx) - softmax 函数
